{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email\n",
    "\n",
    "## How do I Complete this Project?\n",
    "\n",
    "This project is connected to the Intro to Machine Learning course, but depending on your background knowledge of machine learning, you may not need to take the whole thing to complete this project.\n",
    "\n",
    "A note before you begin: the mini-projects in the Intro to Machine Learning class were mostly designed to have lots of data points, give intuitive results, and otherwise behave nicely. This project is significantly tougher in that we're now using the real data, which can be messy and does not have as many data points as we usually hope for when doing machine learning. Don't get discouraged--imperfect data is something you need to be used to as a data analyst! If you encounter something you haven't seen before, take a step back and think about a smart way around. You can do it!\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. In this project, you will play detective, and put your new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. To assist you in your detective work, we've combined this data with a hand-generated list of persons of interest in the fraud case, which means individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "## Resources Needed\n",
    "\n",
    "You should have python and sklearn running on your computer, as well as the starter code (both python scripts and the Enron dataset) that you downloaded as part of the first mini-project in the Intro to Machine Learning course. You can get the starter code on git: ```git clone https://github.com/udacity/ud120-projects.git```\n",
    "\n",
    "The starter code can be found in the final_project directory of the codebase that you downloaded for use with the mini-projects. Some relevant files: \n",
    "\n",
    "**poi_id.py** : Starter code for the POI identifier, you will write your analysis here. You will also submit a version of this file for your evaluator to verify your algorithm and results. \n",
    "\n",
    "**final_project_dataset.pkl** : The dataset for the project, more details below. \n",
    "\n",
    "**tester.py**: When you turn in your analysis for evaluation by Udacity, you will submit the algorithm, dataset and list of features that you use (these are created automatically in ```poi_id.py```). The evaluator will then use this code to test your result, to make sure we see performance that’s similar to what you report. You don’t need to do anything with this code, but we provide it for transparency and for your reference. \n",
    "\n",
    "**emails_by_address** : this directory contains many text files, each of which contains all the messages to or from a particular email address. It is for your reference, if you want to create more advanced features based on the details of the emails dataset. You do not need to process the e-mail corpus in order to complete the project.\n",
    "\n",
    "## Steps to Success\n",
    "\n",
    "We will provide you with starter code that reads in the data, takes your features of choice, then puts them into a numpy array, which is the input form that most sklearn functions assume. Your job is to engineer the features, pick and tune an algorithm, and to test and evaluate your identifier. Several of the mini-projects were designed with this final project in mind, so be on the lookout for ways to use the work you’ve already done.\n",
    "\n",
    "As preprocessing to this project, we've combined the Enron email and financial data into a dictionary, where each key-value pair in the dictionary corresponds to one person. The dictionary key is the person's name, and the value is another dictionary, which contains the names of all the features and their values for that person. The features in the data fall into three major types, namely financial features, email features and POI labels.\n",
    "\n",
    "**financial features**: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
    "\n",
    "**email features**: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)\n",
    "\n",
    "**POI label**: [‘poi’] (boolean, represented as integer)\n",
    "\n",
    "You are encouraged to make, transform or rescale new features from the starter features.  I think this means transforming / combining the features into new formats.  Examples could include creating a bonus / salary feature or a from_poi_to_this_person / from_messages feature.  You're right, this is what they are looking for. If you do this, you should store the new feature to my_dataset, and if you use the new feature in the final algorithm, you should also add the feature name to my_feature_list, so your evaluator can access it during testing.  Add new feature names to features_list in Task 1 and add new features to my_dataset in Task 3.  For a concrete example of a new feature that you could add to the dataset, refer to the lesson on Feature Selection.\n",
    "\n",
    "In addition, we advise that you keep notes as you work through the project. As part of your project submission, you will compose answers to a [series of questions](https://docs.google.com/document/d/1NDgi1PrNJP7WTbfSUuRUnz8yzs5nGVTSzpO7oeNTEWA/pub?embedded=true) (also given on the next page) to understand your approach towards different aspects of the analysis. Your thought process is, in many ways, more important than your final project and we will by trying to probe your thought process in these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV for Exploratory Data Analysis in R\n",
    "\n",
    "The following code will convert the data dictionary into a csv for EDA in R.  It's a lot easier to manipulate and explore the data visually in R.  After getting a better feel for the data, I will be in a better position to eliminate outliers and analyse possible new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodecsv\n",
    "\n",
    "with open('enron_data.csv', 'wb') as csvfile:\n",
    "    fieldnames = ['name', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', \n",
    "                  'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', \n",
    "                  'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', \n",
    "                  'from_this_person_to_poi', 'poi', 'director_fees', 'deferred_income', 'long_term_incentive', \n",
    "                  'email_address', 'from_poi_to_this_person']\n",
    "    writer = unicodecsv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for key, value in data_dict.items():\n",
    "        row = {'name': key}\n",
    "        row.update(value)\n",
    "        writer.writerow(row)\n",
    "        #writer.writerow([{'name': key}, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues to Look Into\n",
    "\n",
    "Getting all features and labels in the right format for machine learning.  Currently throwing errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features\n",
    "\n",
    "The following features may be useful:\n",
    "\n",
    "- fraction_from_this_person_to_poi\n",
    "- fraction_from_poi_to_this_person\n",
    "- bonus/salary\n",
    "- exercised_stock_options / total_stock_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Features\n",
    "\n",
    "The following features showed clear boundaries that may be useful for the classifier:\n",
    "1. director_fees: uniformly non-poi\n",
    "2. restricted_stock_deferred: uniformly non-poi\n",
    "3. fract_to_poi: uniformly non-poi .15 and under\n",
    "4. fract_from_poi: uniformly non-poi around .02 and under\n",
    "5. exercised_stock_options: \n",
    "  1. Uniformly non-poi under \\$300,000\n",
    "  2. Uniformly poi over around \\$20,000,000\n",
    "6. expenses: uniformly non-poi under \\$10,000\n",
    "7. total_stock_value:\n",
    "  1. Small pocket of pois over around $20,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to Definitely Remove\n",
    "\n",
    "1. loan_advances: there are only 3 data points, the rest are missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "### Report\n",
    "\n",
    "Congratulations! You completed the analysis and your evaluation metrics are looking good.  The next thing you need to do is tie up a couple loose ends and write the report:\n",
    "\n",
    "1. Use pipeline for your PCA\n",
    "2. Wanted to create modify_features.py but it looks like you need to do everything in poi.py\n",
    "  1. Add all necessary scripts / functions to poi.py\n",
    "3. Answer questions in report\n",
    "4. Research DecisionTreeClassifier\n",
    "5. Review answers at a separate time and submit\n",
    "\n",
    "\n",
    "### Outliers?\n",
    "- Create an outlier removal function, similar to what you did to remove regression outliers.\n",
    " - Create list of tuples of regression\n",
    " - Remove 10%\n",
    "- Remove outliers from all variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(data_dict):\n",
    "    for key, value in data_dict.items():\n",
    "        from_person_to_poi = value['from_this_person_to_poi']\n",
    "        from_messages = value['from_messages']\n",
    "        from_poi_to_this_person = value['from_poi_to_this_person']\n",
    "        to_messages = value['to_messages']\n",
    "        salary = value['salary']\n",
    "        bonus = value['bonus']\n",
    "        exercised_stock_options = value['exercised_stock_options']\n",
    "        total_stock_value = value['total_stock_value']\n",
    "        \n",
    "        if from_person_to_poi != 'NaN' and from_messages != 'NaN':\n",
    "            value['fract_to_poi'] = float(from_person_to_poi)/from_messages\n",
    "        else:\n",
    "            value['fract_to_poi'] = 'NaN'\n",
    "        if from_poi_to_this_person != 'NaN' and to_messages != 'NaN':\n",
    "            value['fract_from_poi'] = float(from_poi_to_this_person)/to_messages\n",
    "        else:\n",
    "            value['fract_from_poi'] = 'NaN'\n",
    "        if salary != 'NaN' and bonus != 'NaN':\n",
    "            value['salary_over_bonus'] = float(salary)/bonus\n",
    "        else:\n",
    "            value['salary_over_bonus'] = 'NaN'\n",
    "        if exercised_stock_options != 'NaN' and total_stock_value != 'NaN' and total_stock_value != 0:\n",
    "            value['exer_stock_opts_over_tot'] = float(exercised_stock_options)/total_stock_value\n",
    "        else:\n",
    "            value['exer_stock_opts_over_tot'] = 'NaN'\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Features\n",
    "\n",
    "In order to scale the features you're going to need to deal with the dictionary and list of list formats.  You'll probably need to create some sort of numpy array or pandas dataframe out of the dictionary.  Then you can probably run a MinMaxScaler on each column.  Finally, you'll need to place those values back into the dictionary.  This looks like a job for tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scale_features(data_dict):\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    # Create dataframe from dictionary\n",
    "    data_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "    data_df.replace('NaN', np.nan, inplace = True)\n",
    "    \n",
    "    # Scale using manual scaler to ignore non-numerical values\n",
    "    scaled_df = data_df.apply(manual_MinMaxScaler)\n",
    "    \n",
    "    # Return NaN values to original form and create .csv to check\n",
    "    scaled_df.replace(np.nan, 'NaN', inplace = True)\n",
    "    scaled_df.to_csv('scaled_enron_data.csv')\n",
    "    return pd.DataFrame.to_dict(scaled_df, orient='index')\n",
    "    \n",
    "def manual_MinMaxScaler(df):\n",
    "    if df.name != 'name' and df.name != 'email_address' and df.name != 'poi':\n",
    "        min_val = df.min()\n",
    "        max_val = df.max()\n",
    "        return (df-min_val)/(max_val-min_val)\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train shape: <type 'list'>\n",
      "features_test shape: 44\n"
     ]
    }
   ],
   "source": [
    "# final_project/poi_id.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "#sys.path.append(\"../tools/\")\n",
    "\n",
    "#from feature_format import featureFormat, targetFeatureSplit\n",
    "#from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 0: Define functions I'll use since everything is in one script \n",
    "\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'bonus', 'deferral_payments', 'deferred_income', 'director_fees', \n",
    "                 'exer_stock_opts_over_tot', 'exercised_stock_options', 'expenses', \n",
    "                 'fract_from_poi', 'fract_to_poi', 'from_messages', \n",
    "                 'from_poi_to_this_person', 'from_this_person_to_poi', \n",
    "                 'long_term_incentive', 'other', 'restricted_stock', \n",
    "                 'restricted_stock_deferred', 'salary_over_bonus', 'shared_receipt_with_poi', \n",
    "                 'to_messages', 'total_payments', 'total_stock_value'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "dataset_path = \"../udacity/p5-intro-to-machine-learning/ud120-projects-master/final_project/final_project_dataset.pkl\"\n",
    "with open(dataset_path, \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL')\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "\n",
    "new_data_dict = create_features(data_dict)\n",
    "new_data_dict = scale_features(new_data_dict)\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = new_data_dict\n",
    "\n",
    "\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "'''\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(len(labels), 10)\n",
    "for train_indices, test_indeces in kf:\n",
    "    features_train = [feature_data[ii] for ii in train_indices]\n",
    "    features_test = [feature_data[ii] for ii in test_indices]\n",
    "    labels_train = [label_data[ii] for ii in train_indices]\n",
    "    labels_test = [label_data[ii] for ii in test_indices]\n",
    "'''\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Components: 12\n",
      "Explained Variance Ratio: [ 0.28815227  0.20227818  0.15334676  0.07906356]\n",
      "Fitting the classifier to the training set\n",
      "done in 0.017s\n",
      "Best estimator found by grid search:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='best')\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.97      0.96        39\n",
      "        1.0       0.75      0.60      0.67         5\n",
      "\n",
      "avg / total       0.93      0.93      0.93        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Classifiers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 12\n",
    "\n",
    "pca = PCA(n_components = n_components).fit(features_train)\n",
    "print \"Number of Components:\", n_components\n",
    "print \"Explained Variance Ratio:\", pca.explained_variance_ratio_[:4]\n",
    "#print \"Components:\", pca.components_\n",
    "features_train_pca = pca.transform(features_train)\n",
    "features_test_pca = pca.transform(features_test)\n",
    "\n",
    "# Train a classification model using the new PCA\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {\n",
    "         'min_samples_split': [2],\n",
    "        'random_state': [42]\n",
    "          }\n",
    "# for sklearn version 0.16 or prior, the class_weight parameter value is 'auto'\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "clf = clf.fit(features_train_pca, labels_train)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "clf = DecisionTreeClassifier(min_samples_split = 4, random_state = 42)\n",
    "print \"Length of features_train_pca:\", len(features_train_pca)\n",
    "print \"Length of labels_train:\", len(labels_train)\n",
    "\n",
    "clf.fit(features_train_pca, labels_train)\n",
    "#print \"Best Estimator:\", clf.best_estimator_\n",
    "'''\n",
    "\n",
    "# Test out the classification model\n",
    "pred = clf.predict(features_test_pca)\n",
    "from sklearn.metrics import classification_report\n",
    "print \"Classification Report:\"\n",
    "print classification_report(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "# final_project/tester.py\n",
    "\n",
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "#sys.path.append(\"../tools/\")\n",
    "#from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "my_path = '../udacity/p5-intro-to-machine-learning/ud120-projects-master/final_project/'\n",
    "        \n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(my_path + CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(my_path + DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(my_path + FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(my_path + CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(my_path + DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(my_path + FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Results Log\n",
    "\n",
    "## Features used:\n",
    "\n",
    "features_list = ['poi','salary', 'fract_to_poi', 'fract_from_poi',\n",
    "                'director_fees', 'restricted_stock_deferred', 'exercised_stock_options',\n",
    "                'expenses', 'total_stock_value']\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-21 at 5.48.50 PM.png'>\n",
    "\n",
    "### DecisionTree\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-21 at 5.50.06 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-21 at 5.51.45 PM.png'>\n",
    "\n",
    "### KNearest Neighbors\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-21 at 5.57.06 PM.png'>\n",
    "\n",
    "\n",
    "## PCA\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-22 at 11.10.54 AM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-22 at 11.34.58 AM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-22 at 11.36.44 AM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.18.59 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.20.39 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.21.01 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.21.15 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.22.00 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.22.21 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.52.49 PM.png'>\n",
    "\n",
    "### random_state = 42\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 1.01.54 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/'>\n",
    "\n",
    "<img src = 'screenshots/'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tools/feature_format.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknown Code:\n",
    "\n",
    "The following code blocks may or may not be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tools/email_preprocess.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import cPickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(words_file = \"../tools/word_data.pkl\", authors_file=\"../tools/email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    ### this preprocessing will be repeated in the text learning mini-project\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = pickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = model_selection.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    selector = SelectPercentile(f_classif, percentile=1)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tools/parse_out_email_text.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        #words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "        stemmer = SnowballStemmer('english')\n",
    "\n",
    "        for word in text_string.split():\n",
    "        \tstemmed_word = stemmer.stem(word)\n",
    "        \twords += \" \" + stemmed_word\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText(ff)\n",
    "    print text\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
