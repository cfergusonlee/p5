{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Identify Fraud from Enron Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Issues to Look Into\n",
    "\n",
    "Getting all features and labels in the right format for machine learning.  Currently throwing errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## New Features\n",
    "\n",
    "The following features may be useful:\n",
    "\n",
    "- fraction_from_this_person_to_poi\n",
    "- fraction_from_poi_to_this_person\n",
    "- bonus/salary\n",
    "- exercised_stock_options / total_stock_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Important Features\n",
    "\n",
    "The following features showed clear boundaries that may be useful for the classifier:\n",
    "1. director_fees: uniformly non-poi\n",
    "2. restricted_stock_deferred: uniformly non-poi\n",
    "3. fract_to_poi: uniformly non-poi .15 and under\n",
    "4. fract_from_poi: uniformly non-poi around .02 and under\n",
    "5. exercised_stock_options: \n",
    "  1. Uniformly non-poi under \\$300,000\n",
    "  2. Uniformly poi over around \\$20,000,000\n",
    "6. expenses: uniformly non-poi under \\$10,000\n",
    "7. total_stock_value:\n",
    "  1. Small pocket of pois over around $20,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Features to Definitely Remove\n",
    "\n",
    "1. loan_advances: there are only 3 data points, the rest are missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Todo\n",
    "\n",
    "### Report\n",
    "\n",
    "Congratulations! You completed the analysis and your evaluation metrics are looking good.  The next thing you need to do is tie up a couple loose ends and write the report:\n",
    "\n",
    "1. Use pipeline for your PCA\n",
    "2. Wanted to create modify_features.py but it looks like you need to do everything in poi.py\n",
    "  1. Add all necessary scripts / functions to poi.py\n",
    "3. Answer questions in report\n",
    "4. Research DecisionTreeClassifier\n",
    "5. Review answers at a separate time and submit\n",
    "\n",
    "\n",
    "### Outliers?\n",
    "- Create an outlier removal function, similar to what you did to remove regression outliers.\n",
    " - Create list of tuples of regression\n",
    " - Remove 10%\n",
    "- Remove outliers from all variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_features(data_dict):\n",
    "    for key, value in data_dict.items():\n",
    "        from_person_to_poi = value['from_this_person_to_poi']\n",
    "        from_messages = value['from_messages']\n",
    "        from_poi_to_this_person = value['from_poi_to_this_person']\n",
    "        to_messages = value['to_messages']\n",
    "        salary = value['salary']\n",
    "        bonus = value['bonus']\n",
    "        exercised_stock_options = value['exercised_stock_options']\n",
    "        total_stock_value = value['total_stock_value']\n",
    "        \n",
    "        if from_person_to_poi != 'NaN' and from_messages != 'NaN':\n",
    "            value['fract_to_poi'] = float(from_person_to_poi)/from_messages\n",
    "        else:\n",
    "            value['fract_to_poi'] = 'NaN'\n",
    "        if from_poi_to_this_person != 'NaN' and to_messages != 'NaN':\n",
    "            value['fract_from_poi'] = float(from_poi_to_this_person)/to_messages\n",
    "        else:\n",
    "            value['fract_from_poi'] = 'NaN'\n",
    "        if salary != 'NaN' and bonus != 'NaN':\n",
    "            value['salary_over_bonus'] = float(salary)/bonus\n",
    "        else:\n",
    "            value['salary_over_bonus'] = 'NaN'\n",
    "        if exercised_stock_options != 'NaN' and total_stock_value != 'NaN' and total_stock_value != 0:\n",
    "            value['exer_stock_opts_over_tot'] = float(exercised_stock_options)/total_stock_value\n",
    "        else:\n",
    "            value['exer_stock_opts_over_tot'] = 'NaN'\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Scaling Features\n",
    "\n",
    "In order to scale the features you're going to need to deal with the dictionary and list of list formats.  You'll probably need to create some sort of numpy array or pandas dataframe out of the dictionary.  Then you can probably run a MinMaxScaler on each column.  Finally, you'll need to place those values back into the dictionary.  This looks like a job for tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def scale_features(data_dict):\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    # Create dataframe from dictionary\n",
    "    data_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "    data_df.replace('NaN', np.nan, inplace = True)\n",
    "    \n",
    "    # Scale using manual scaler to ignore non-numerical values\n",
    "    scaled_df = data_df.apply(manual_MinMaxScaler)\n",
    "    \n",
    "    # Transform using PCA\n",
    "    #transformed_df = scaled_df.apply(transform_PCA)\n",
    "    \n",
    "    # Return NaN values to original form and create .csv to check\n",
    "    scaled_df.replace(np.nan, 'NaN', inplace = True)\n",
    "    scaled_df.to_csv('scaled_enron_data.csv')\n",
    "    return pd.DataFrame.to_dict(scaled_df, orient='index')\n",
    "    \n",
    "def manual_MinMaxScaler(df):\n",
    "    if df.name != 'name' and df.name != 'email_address' and df.name != 'poi':\n",
    "        min_val = df.min()\n",
    "        max_val = df.max()\n",
    "        return (df-min_val)/(max_val-min_val)\n",
    "    else:\n",
    "        return df\n",
    "'''\n",
    "def transform_PCA(data_df):\n",
    "    # Need to transform data using pca\n",
    "    # input = np array \n",
    "    # output = np array of formatted features\n",
    "    # Need to \n",
    "    data_np = data_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier\n",
      "Training time: 0.128s\n",
      "Pipeline(steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=7, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='best'))])\n",
      "Explained variance ratio: [ 0.3306276   0.21620042  0.1924513   0.10458867  0.08131957  0.07125716\n",
      "  0.00355528]\n",
      "Feature importances: [ 0.06752799  0.          0.14244186  0.55146011  0.20164066  0.03692937\n",
      "  0.        ]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.95      0.92        37\n",
      "        1.0       0.50      0.33      0.40         6\n",
      "\n",
      "avg / total       0.84      0.86      0.85        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final_project/poi_id.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "#sys.path.append(\"../tools/\")\n",
    "\n",
    "#from feature_format import featureFormat, targetFeatureSplit\n",
    "#from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 0: Define functions I'll use since everything is in one script \n",
    "\n",
    "def create_features(data_dict):\n",
    "    for key, value in data_dict.items():\n",
    "        from_person_to_poi = value['from_this_person_to_poi']\n",
    "        from_messages = value['from_messages']\n",
    "        from_poi_to_this_person = value['from_poi_to_this_person']\n",
    "        to_messages = value['to_messages']\n",
    "        salary = value['salary']\n",
    "        bonus = value['bonus']\n",
    "        exercised_stock_options = value['exercised_stock_options']\n",
    "        total_stock_value = value['total_stock_value']\n",
    "        \n",
    "        if from_person_to_poi != 'NaN' and from_messages != 'NaN':\n",
    "            value['fract_to_poi'] = float(from_person_to_poi)/from_messages\n",
    "        else:\n",
    "            value['fract_to_poi'] = 'NaN'\n",
    "        if from_poi_to_this_person != 'NaN' and to_messages != 'NaN':\n",
    "            value['fract_from_poi'] = float(from_poi_to_this_person)/to_messages\n",
    "        else:\n",
    "            value['fract_from_poi'] = 'NaN'\n",
    "        if salary != 'NaN' and bonus != 'NaN':\n",
    "            value['salary_over_bonus'] = float(salary)/bonus\n",
    "        else:\n",
    "            value['salary_over_bonus'] = 'NaN'\n",
    "        if exercised_stock_options != 'NaN' and total_stock_value != 'NaN' and total_stock_value != 0:\n",
    "            value['exer_stock_opts_over_tot'] = float(exercised_stock_options)/total_stock_value\n",
    "        else:\n",
    "            value['exer_stock_opts_over_tot'] = 'NaN'\n",
    "    return data_dict\n",
    "\n",
    "def scale_features(data_dict):\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    # Create dataframe from dictionary\n",
    "    data_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "    data_df.replace('NaN', np.nan, inplace = True)\n",
    "    \n",
    "    # Scale using manual scaler to ignore non-numerical values\n",
    "    scaled_df = data_df.apply(manual_MinMaxScaler)\n",
    "    \n",
    "    # Return NaN values to original form and create .csv to check\n",
    "    scaled_df.replace(np.nan, 'NaN', inplace = True)\n",
    "    scaled_df.to_csv('scaled_enron_data.csv')\n",
    "    return pd.DataFrame.to_dict(scaled_df, orient='index')\n",
    "    \n",
    "def manual_MinMaxScaler(df):\n",
    "    if df.name != 'name' and df.name != 'email_address' and df.name != 'poi':\n",
    "        min_val = df.min()\n",
    "        max_val = df.max()\n",
    "        return (df-min_val)/(max_val-min_val)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "'''\n",
    "features_list = ['poi', 'bonus', 'deferral_payments', 'deferred_income', 'director_fees', \n",
    "                 'exer_stock_opts_over_tot', 'exercised_stock_options', 'expenses', \n",
    "                 'fract_from_poi', 'fract_to_poi', 'from_messages', \n",
    "                 'from_poi_to_this_person', 'from_this_person_to_poi', \n",
    "                 'long_term_incentive', 'other', 'restricted_stock', \n",
    "                 'restricted_stock_deferred', 'salary_over_bonus', 'shared_receipt_with_poi', \n",
    "                 'to_messages', 'total_payments', 'total_stock_value']\n",
    "'''\n",
    "\n",
    "features_list = ['poi', 'fract_from_poi', 'fract_to_poi', 'from_messages', 'director_fees',\n",
    "                 'restricted_stock_deferred', 'exercised_stock_options', 'expenses']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "dataset_path = \"../udacity/p5-intro-to-machine-learning/ud120-projects-master/final_project/final_project_dataset.pkl\"\n",
    "with open(dataset_path, \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL')\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "\n",
    "new_data_dict = create_features(data_dict)\n",
    "\n",
    "### Scale features for use in PCA\n",
    "new_data_dict = scale_features(new_data_dict)\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = new_data_dict\n",
    "\n",
    "\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info:\n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 12\n",
    "\n",
    "skb = SelectKBest(f_classif, k = 16)\n",
    "pca = PCA(random_state = 42)#.fit(features_train)\n",
    "clf_DT = DecisionTreeClassifier(random_state=42)\n",
    "clf_AB = AdaBoostClassifier()\n",
    "clf_SVM = SVC()\n",
    "steps = [('PCA', pca),\n",
    "         ('clf', clf_DT)]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "params = {\"PCA__n_components\": [2, 3, 4, 5, 6, 7]}\n",
    "          #\"clf__kernel\": ['linear', 'rbf', 'poly']}\n",
    "\n",
    "gs = GridSearchCV(pipe, \n",
    "                  param_grid = params, \n",
    "                  scoring = 'f1_weighted')\n",
    "\n",
    "\n",
    "print \"Training the classifier\"\n",
    "t0 = time()\n",
    "gs.fit(features_train, labels_train)\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "print \"Training time: %0.3fs\" % (time() - t0)\n",
    "pred = clf.predict(features_test)\n",
    "print clf\n",
    "print \"Explained variance ratio:\", gs.best_estimator_.named_steps['PCA'].explained_variance_ratio_\n",
    "print \"Feature importances:\", gs.best_estimator_.named_steps['clf'].feature_importances_\n",
    "print classification_report(labels_test, pred)\n",
    "\n",
    "#print \"Number of Components:\", n_components\n",
    "#print \"Explained Variance Ratio:\", pca.explained_variance_ratio_[:4]\n",
    "\n",
    "#features_train_pca = pca.transform(features_train)\n",
    "#features_test_pca = pca.transform(features_test)\n",
    "\n",
    "# Train a classification model using the new PCA\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from time import time\n",
    "\n",
    "#clf = GridSearchCV(pipe, scoring = 'f1_weighted')\n",
    "\n",
    "'''\n",
    "skb = SelectKBest(f_classif)\n",
    "pca = PCA()\n",
    "clf_AB = AdaBoostClassifier()\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "param_grid = {\n",
    "         'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'random_state': [42]\n",
    "          }\n",
    "\n",
    "pca_params = {\"PCA__n_components\": [8],\n",
    "             \"PCA__whiten\": [True],\n",
    "             \"SKB__k\": [8, 10]}\n",
    "\n",
    "steps = [('SKB', skb),\n",
    "         ('PCA', pca),\n",
    "         ('clf', clf_DT)]\n",
    "\n",
    "clf = GridSearchCV(pipe,\n",
    "                  pca_params,\n",
    "                  verbose = 0,\n",
    "                  scoring = 'f1_weighted')\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "'''\n",
    "\n",
    "'''\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {\n",
    "         'min_samples_split': [2],\n",
    "        'random_state': [42]\n",
    "          }\n",
    "# for sklearn version 0.16 or prior, the class_weight parameter value is 'auto'\n",
    "'''\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=7, random_state=42,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='best'))])\n",
      "\tAccuracy: 0.82020\tPrecision: 0.34627\tRecall: 0.39250\tF1: 0.36794\tF2: 0.38229\n",
      "\tTotal predictions: 15000\tTrue positives:  785\tFalse positives: 1482\tFalse negatives: 1215\tTrue negatives: 11518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/pickle\n",
    "\n",
    "\"\"\" a basic script for importing student's POI identifier,\n",
    "    and checking the results that they get from it \n",
    " \n",
    "    requires that the algorithm, dataset, and features list\n",
    "    be written to my_classifier.pkl, my_dataset.pkl, and\n",
    "    my_feature_list.pkl, respectively\n",
    "\n",
    "    that process should happen at the end of poi_id.py\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "#sys.path.append(\"../tools/\")\n",
    "#from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "my_path = '../udacity/p5-intro-to-machine-learning/ud120-projects-master/final_project/'\n",
    "        \n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(my_path + CLF_PICKLE_FILENAME, \"w\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(my_path + DATASET_PICKLE_FILENAME, \"w\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(my_path + FEATURE_LIST_FILENAME, \"w\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(my_path + CLF_PICKLE_FILENAME, \"r\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(my_path + DATASET_PICKLE_FILENAME, \"r\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(my_path + FEATURE_LIST_FILENAME, \"r\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create CSV for Exploratory Data Analysis in R\n",
    "\n",
    "The following code will convert the data dictionary into a csv for EDA in R.  It's a lot easier to manipulate and explore the data visually in R.  After getting a better feel for the data, I will be in a better position to eliminate outliers and analyse possible new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import unicodecsv\n",
    "\n",
    "with open('enron_data.csv', 'wb') as csvfile:\n",
    "    fieldnames = ['name', 'salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', \n",
    "                  'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', \n",
    "                  'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', \n",
    "                  'from_this_person_to_poi', 'poi', 'director_fees', 'deferred_income', 'long_term_incentive', \n",
    "                  'email_address', 'from_poi_to_this_person']\n",
    "    writer = unicodecsv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    for key, value in data_dict.items():\n",
    "        row = {'name': key}\n",
    "        row.update(value)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import Classifiers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 12\n",
    "\n",
    "pca = PCA(n_components = n_components).fit(features_train)\n",
    "print \"Number of Components:\", n_components\n",
    "print \"Explained Variance Ratio:\", pca.explained_variance_ratio_[:4]\n",
    "\n",
    "features_train_pca = pca.transform(features_train)\n",
    "features_test_pca = pca.transform(features_test)\n",
    "\n",
    "# Train a classification model using the new PCA\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "print \"Fitting the classifier to the training set\"\n",
    "t0 = time()\n",
    "param_grid = {\n",
    "         'min_samples_split': [2],\n",
    "        'random_state': [42]\n",
    "          }\n",
    "# for sklearn version 0.16 or prior, the class_weight parameter value is 'auto'\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "clf = clf.fit(features_train_pca, labels_train)\n",
    "print \"done in %0.3fs\" % (time() - t0)\n",
    "print \"Best estimator found by grid search:\"\n",
    "print clf.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "clf = DecisionTreeClassifier(min_samples_split = 4, random_state = 42)\n",
    "\n",
    "clf.fit(features_train_pca, labels_train)\n",
    "#print \"Best Estimator:\", clf.best_estimator_\n",
    "'''\n",
    "\n",
    "# Test out the classification model\n",
    "pred = clf.predict(features_test_pca)\n",
    "from sklearn.metrics import classification_report\n",
    "print \"Classification Report:\"\n",
    "print classification_report(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classifier Results Log\n",
    "\n",
    "## Features used:\n",
    "\n",
    "features_list = ['poi','salary', 'fract_to_poi', 'fract_from_poi',\n",
    "                'director_fees', 'restricted_stock_deferred', 'exercised_stock_options',\n",
    "                'expenses', 'total_stock_value']\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-21 at 5.48.50 PM.png'>\n",
    "\n",
    "### DecisionTree\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-21 at 5.50.06 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-21 at 5.51.45 PM.png'>\n",
    "\n",
    "### KNearest Neighbors\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-21 at 5.57.06 PM.png'>\n",
    "\n",
    "\n",
    "## PCA\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-22 at 11.10.54 AM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-22 at 11.34.58 AM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-22 at 11.36.44 AM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.18.59 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.20.39 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.21.01 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.21.15 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.22.00 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.22.21 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 12.52.49 PM.png'>\n",
    "\n",
    "### random_state = 42\n",
    "\n",
    "<img src = 'screenshots/Screen Shot 2017-06-23 at 1.01.54 PM.png'>\n",
    "\n",
    "<img src = 'screenshots/'>\n",
    "\n",
    "<img src = 'screenshots/'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tools/feature_format.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    A general tool for converting data from the\n",
    "    dictionary format to an (n x k) python list that's \n",
    "    ready for training an sklearn algorithm\n",
    "\n",
    "    n--no. of key-value pairs in dictonary\n",
    "    k--no. of features being extracted\n",
    "\n",
    "    dictionary keys are names of persons in dataset\n",
    "    dictionary values are dictionaries, where each\n",
    "        key-value pair in the dict is the name\n",
    "        of a feature, and its value for that person\n",
    "\n",
    "    In addition to converting a dictionary to a numpy \n",
    "    array, you may want to separate the labels from the\n",
    "    features--this is what targetFeatureSplit is for\n",
    "\n",
    "    so, if you want to have the poi label as the target,\n",
    "    and the features you want to use are the person's\n",
    "    salary and bonus, here's what you would do:\n",
    "\n",
    "    feature_list = [\"poi\", \"salary\", \"bonus\"] \n",
    "    data_array = featureFormat( data_dictionary, feature_list )\n",
    "    label, features = targetFeatureSplit(data_array)\n",
    "\n",
    "    the line above (targetFeatureSplit) assumes that the\n",
    "    label is the _first_ item in feature_list--very important\n",
    "    that poi is listed first!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        import pickle\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Unknown Code:\n",
    "\n",
    "The following code blocks may or may not be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tools/email_preprocess.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import cPickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(words_file = \"../tools/word_data.pkl\", authors_file=\"../tools/email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    ### this preprocessing will be repeated in the text learning mini-project\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = pickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = model_selection.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    selector = SelectPercentile(f_classif, percentile=1)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tools/parse_out_email_text.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        #words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "        stemmer = SnowballStemmer('english')\n",
    "\n",
    "        for word in text_string.split():\n",
    "        \tstemmed_word = stemmer.stem(word)\n",
    "        \twords += \" \" + stemmed_word\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "    \n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText(ff)\n",
    "    print text\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
