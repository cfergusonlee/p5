---
title: "Identifying Fraud from Enron Emails"
author: "By: Courtney Ferguson Lee"
date: "June 29, 2017"
output: html_document
---

<style type="text/css">
body{ /* Normal  */
      font-size: 18px;
  }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      message = F,
                      warning = F,
                      fig.align = 'center')
```

```{r Load_Packages}
library(tidyverse)
library(ggplot2)
library(scales) # need to get labels = comma
library(GGally)
```

```{r Load_Data}
setwd('/Users/courtneyfergusonlee/p5')
enron <- read.csv('enron_data.csv', 
                  na.strings = 'NaN',
                  stringsAsFactors = F)
```

```{r Reshape_Data}
enron.new_features <- enron %>%
  mutate(fract_to_poi = from_this_person_to_poi/from_messages,
         fract_from_poi = from_poi_to_this_person/to_messages,
         exer_stock_opts_over_tot = exercised_stock_options/total_stock_value,
         poi = as.factor(poi))
```



## Introduction

The goal for this project was to identify persons of interest (POI's) from a 
dataset of Enron employees.  The dataset initially contained 146 observations 
across 21 features and a POI status label.  There were 18 POI's and 128 
non-POI's.  I used a supervised machine learning algorithm to identify patterns 
in the email and financial data that separated POI's from non-POI's. The financial
and email variables included:

Financial Variables | Email Variables
---|---
bonus | email_address
deferral_payments | from_messages
deferred_income | from_poi_to_this_person
director_fees | from_this_person_to_poi
exer_stock_opts_over_tot | shared_receipt_with_poi
exercised_stock_options | to_messages
expenses | 
loan_advances | 
long_term_incentive | 
other | 
restricted_stock | 
restricted_stock_deferred | 
salary | 
salary_over_bonus | 
total_payments | 
total_stock_value | 
  

Three outliers in the dataset needed to be removed: “TOTAL”, “THE TRAVEL AGENCY 
IN THE PARK”, and "LOCKHART EUGENE E".  The first two weren't actual people, 
so they did not contribute to the model.  Eugene Lockhart was missing data for
all of his features, so he was removed as well.  Because of the nature of the
dataset, it was difficult to remove any outliers that varied significantly from 
the rest of the population. The dataset consisted of less than 150 people and 
there was a lot of missing data, as shown in the following chart:


```{r Missing_Values_Heatmap}
enron.missing <- as.data.frame(sapply(enron.new_features, is.na))

enron.long <- enron.new_features %>%
  gather(var_name,
         value,
         -name,
         na.rm = F)

enron.long.missing <- enron.long %>%
  mutate(value = !is.na(value))

ggplot(data = enron.long.missing,
       aes(x = name,
           y = var_name)) +
  geom_tile(aes(fill = value)) +
  scale_fill_manual(values = c('grey39', 'white'),
                    labels = c('Missing', 'Present')) +
  #scale_fill_brewer(palette = "RdYlGn") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  theme(legend.key = element_rect(colour = "black")) +
  ggtitle("Heatmap of Missing Values") +
  xlab("Observations") +
  ylab("Variable Name") +
  labs(fill = "Data Availability")
```

There were several financial and email outliers, but I ultimately included them
because they contained useful information. People like Ken Lay and Joseph Hirko 
had the largest exercised stock options (by several orders of magnitude), but 
they were both persons of interest. With such a small number of observations,
excluding any outliers would have had a significant impact on the model.


## Feature Engineering

I used Exploratory Data Analysis to identify / engineer the 9 features below to 
help create clearer boundaries for my classifier. I created two of the features: 
the fraction of emails sent to POI's and the fraction of emails from POI's.  I 
did this because it did not make sense to use the raw number of emails to or 
from a POI.  The total number of sent and received emails ranged from around 10 
to over 10,000.  To better compare human interactions, it made more sense to 
code this data as a proportion.  When I created the scatterplot of the two 
features (further below), they created clear boundaries that helped improve my 
classifier.The fraction of emails sent to a POI (x-axis) below 16% and above 
68% uniformly contained non-POI's. The same was true for the fraction of emails 
from a POI (y-axis) below 2% and above 14%.  These clear boundaries helped my
classifier distinguish between POI's and non-POI's.

Variable | Definition
----------------- | -------------------------------------------------
salary | Guaranteed annual salary for each employee
bonus | Additional compensation tied to an employee's performance review
total_stock_value | Total value of Enron stock an employee received as compensation
fract_from_poi | Fraction of total emails received from a poi
fract_to_poi | Fraction of total emails sent to a poi
director_fees | Compensation for attendance at board meetings
restricted_stock_deferred | Stock that was not fully transferrable until certain conditions were met
exercised_stock_options | Amount of Enron stock an employee bought or sold
expenses |  Costs occurred by an employee while conducting business


```{r}
ggplot(data = enron.new_features,
       aes(x = fract_to_poi,
           y = fract_from_poi,
           color = poi)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 1.00, .1)) +
  scale_y_continuous(breaks = seq(0, .23, .01)) +
  ggtitle('Fraction of Emails From POI vs Fraction of Emails to POI') +
  xlab('Fraction Sent to POI') +
  ylab('Fraction From POI') +
  labs(color = 'POI')
```


I also used SelectKBest to help determine which features contributed the most
variance in the data.  I plotted a negative log of the p-values below.  This
helped highlight a few features I initially overlooked when completing my 
analysis. In particular, the bonus and total_stock_value features jumped out at
me as important variables to consider. The following plot shows each feature's 
relative impact:

<img src = 'screenshots/Screen Shot 2017-07-10 at 1.36.08 PM.png'>

Below are the evaluation metrics for my final model with and without the
fract_to_poi and fract_from_poi features.  The results show that after adding the
engineered features to the model, precision increased from 27% to 35% and recall
increased from 44% to 78%.  I also created the variables salary_over_bonus and
exer_stock_opts_over_tot. When I tested them in my algorithm, their evaluation
metrics were better than the original 7, but lower than fract_from_poi and 
fract_to_poi alone, so I omitted them. The results of adding those variables to 
my final algorithm (11 total features) are summarized below:

**Omitting fract_from_poi and fract_to_poi**
<img src = 'screenshots/Screen Shot 2017-07-11 at 1.18.19 AM.png'>

**Including fract_from_poi and fract_to_poi**
<img src = 'screenshots/Screen Shot 2017-07-11 at 1.19.07 AM.png'>

**Including fract_from_poi, fract_to_poi, salary_over_bonus and exer_stock_opts_over_tot**
<img src = 'screenshots/Screen Shot 2017-07-11 at 1.39.44 AM.png'>

The plots below helped me further understand the underlying nature of the 
financial and email variables.  The boxplot of financial variables helped me see
that only Non-POI's had director_fees and restricted_stock_deferred variables 
present. The emails boxplot made me think critically about how I could engineer 
new fractional email features which I eventually included in my classifier:


```{r Financial_long_form}
financial_long <- enron %>%
  mutate(deferred_income = -deferred_income,
         restricted_stock_deferred = -restricted_stock_deferred) %>%
  select(expenses,
         other,
         director_fees,
         restricted_stock_deferred,
         deferred_income,
         deferral_payments,
         salary,
         long_term_incentive,
         restricted_stock,
         bonus,
         total_payments,
         total_stock_value,
         exercised_stock_options,
         loan_advances,
         poi) %>%
  gather(financial_var,
         amount,
         -poi) %>%
  mutate(financial_var = as.factor(financial_var))
```


```{r Financial_Boxplot}
ggplot(data = subset(financial_long, 
                     !is.na(amount) & amount > 0),
       aes(x = reorder(financial_var, 
                       amount, 
                       median),
           y = amount)) +
  geom_boxplot() +
  geom_jitter(aes(color = poi,
                  alpha = .2),
              width = .2,
              size = .7) +
  scale_y_log10(labels = comma,
                breaks = 10^seq(2, 8, 1)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  ggtitle("Distribution of Financial Variables") +
  xlab("Financial Variable") +
  ylab("Amount") +
  guides(alpha = 'none') +
  labs(color = 'POI')

```

```{r Email_Long_Form}
email_long <- enron %>%
  select(to_messages,
         shared_receipt_with_poi,
         from_messages,
         from_this_person_to_poi,
         from_poi_to_this_person,
         poi) %>%
  gather(email_feature,
         amount,
         -poi) %>%
  mutate(email_feature = as.factor(email_feature))
```

```{r Email_Boxplot}
ggplot(data = subset(email_long, !is.na(amount)),
       aes(x = reorder(email_feature, amount, median),
           y = amount + 1)) +
  geom_boxplot() +
  geom_jitter(aes(alpha = .2,
                  color = poi),
              width = .2) +
  scale_y_log10(labels = comma,
                breaks = c(0,
                           10,
                           100,
                           1000,
                           10000)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1)) +
  ggtitle('Distribution of Emails') +
  xlab('Email Feature') +
  ylab('Number of Messages') +
  guides(alpha = 'none') +
  labs(color = 'POI')
```


After identifying useful features, I scaled them before trying PCA in my model. I
scaled the features because the ranges of the variables were too large and would
throw off predictions.  For instance, the range for fractional email variables 
ranged from 0.0 to 1.0 while exercised stock options varied from 1,000 to 
around 30,000,000.  With such a large range, PCA would not be able to properly
determine which variable accounted for most of the variance in the data. I 
created my own MinMaxScaler and applied it right after creating new features.
It ensured that all variables ranged from 0 to 1.  PCA allowed the data to speak
for itself in a way. I also used GridSearchCV to tune the parameters of PCA 
itself.  After testing out a range of 2 to 11 components for PCA I realized that
I had better evaluation metrics without the transformation, so I excluded it. 

## Algorithm Selection

I tested out several algorithms before settling on a Support Vector Machine. I
started with a GaussianNaiveBayes to get a benchmark, then tried out a Decision 
Tree, KNearest Neighbors, AdaBoost and finally Support Vector Machines. The 
KNearest Neighbors Classifier had the worst results overall at only .22 for 
precision, recall and F1 scores.  AdaBoost was slower than the other classifiers 
and did only marginally better than KNearest Neighbors. The Decision Tree 
Classifier initially had the best overall precision, recall and F1 scores
but as we see further below, there was another classifier that performed slightly
better.

Support Vector Machines had the best overall scores once I tuned the
class_weight parameter to "balanced".  This was important because the dataset
was very imbalanced.  There were only 18 POI's and 128 non-POI's (125 once
outliers were removed).  Before tuning the class_weight parameter, the algorithm
penalized mistakes to both classes equally.  Adjusting the class_weight parameter
remedied this. The original survey results are summarized below.  I used the
following features in each model:

- salary
- fract_to_poi
- fract_from_poi
- director_fees
- restricted_stock_deferred
- exercised_stock_options
- expenses
- total_stock_value

Classifier | Results
----- | -----
DecisionTree | <img src= 'screenshots/Screen Shot 2017-06-21 at 5.50.06 PM.png'>
Naive Bayes | <img src = 'screenshots/Screen Shot 2017-06-21 at 5.48.50 PM.png'>
KNearest Neighbors | <img src = 'screenshots/Screen Shot 2017-06-21 at 5.57.06 PM.png'>
SupportVectorMachines | <img src = 'screenshots/Screen Shot 2017-07-09 at 2.13.13 AM.png'>


## Parameter Tuning

Tuning the parameters of an algorithm involves testing out different 
combinations of input arguments.  These input arguments, known as hyperparamters
are set before an algorithm is fit to data and affect the performance on an 
independent data set. These hyperparameters affect the performance of a model,
including but not limited to its complexity, learning capacity, and speed. This
is one way to "custom fit" an algorithm to the training data it is modeling.
Parameter tuning is a way to adjust an algorithm at a high level prior to 
training. For instance, if you are using a DecisionTree Classifier and your
model is underfitting to your data, you can adjust the min_samples_split
hyperparameter to allow the model to make more granular splits and hence more
closely fit the data.

Parameter tuning applies to both pre-processing steps like PCA and the algorithm
chosen.  I used GridSearchCV to accomplish this.  It allowed me to try several 
different parameters in one shot as opposed to rerunning the algorithm every 
time I wanted to make an adjustment.  I tried a range of 2-11 PCA components and 
found that PCA didn't help my model much. When testing the DecisionTree 
classifier, I was able to experiment with different max_depths, split criteria
and min_samples_splits.  We calculated entropy for various scenarios in
the lessons so I tried using that as an alternative criterion, but the results 
didn't change much from baseline.

In the end I stuck with the baseline parameters for SVC. I tested different
C-values, gammas and class weights.  Again, they didn't perform better than 
baseline.  I set the random_state to 42 because the answer is always 42 :). When
I did not specify a random_state, the scores kept bouncing all over the place. 
I realized this was an important step to ensure that others could reproduce my 
results.

## Model Validation

Validation involves ensuring a model is effective on both training and
testing data.  One pitfall in validation is when you accidentally overfit your
model to the training data.  Overfitting is when your model performs well on
training data but poorly on new testing data.  This can happen if you don't 
partition a large enough chunk for your testing set or when you use an
overly-convoluted boundary when training.

I validated my analysis with cross validation using StratifiedShuffleSplit. I
split the data into 10 folds, trained it on each of the 10 folds, then computed
the accuracy scores for each.  This kept the proportion of POI's and non-POI's
as close as possible to the proportion seen in the full dataset. The average
overall accuracy score using this method was 81.2%.

## Conclusion

My final precision and recall scores were 35% and 78%, respectively.  Precision 
is a ratio of true positives to the total number of true positives and false 
positives. In other words, it asks, "Out of all of the items that are truly
positive, how many were correctly labeled as positive?" Recall is a ratio of
true positives to the total number of true positives and false negatives. It 
asks, "Out of all of the items labeled as positive, how many truly belong to the
positive class?" Below are the equations for precision and recall as well as the
evaluation metrics from tester.py:

<img src='https://help.kcura.com/9.4/Content/Resources/Images/Recipes/How_to_Calculate_Precision_and_Recall_without_a_Control_Set/SCR_PrecisionEquation.png' width=400>

<img src='https://help.kcura.com/9.4/Content/Resources/Images/Recipes/How_to_Calculate_Precision_and_Recall_without_a_Control_Set/SCR_RecallEquation_821x99.png' width=400>

<img src ='screenshots/Screen Shot 2017-07-10 at 1.08.39 PM.png'>

In context, my algorithm had more false positives than false negatives.  A
false positive was a situation where a POI was labeled as a Non-POI and a false
negative was a situation where a Non-POI was labeled as a POI.  This led to
high recall scores and low precision scores. I could have used accuracy as my
primary metric, but that would have been misleading.There were only 18 POI's
in the entire dataset. Because the dataset was so imbalanced, I could have 
achieved 87.4% accuracy by simply labeling all observations as Non-POI's. That
is why precision and recall were better metrics for evaluating my algorithm on
this dataset.